User Story 1:
User will place files in a azure blob storage containing SAP product masterdata with additional hierarchy information. 
This masterdata is enhanced and useful for end reporting

The system should :
1) Ingest this masterdata into TMP_Product table. THis table should have same structure as the masterdata
2) Keys to access sharepoint should be stored in Azure Key vault and used from there
3) Create a log table with ext_id, status, date of run, the period of run, yearmonth
4) Update or insert into this log table based on a new run,


USer Story 2:
create a pyspark functions which: 
  parameters raw path, bronze path, required schema, sheet , move, callback
  moves files from raw to bronze path
  if no files are present, exits without error
  use a callback function to read the file
    if it is a koalas dataframe, convert to pyspark
  if it is not callback, read the csv file or excel file
  if a schema is provided ==> validate the provided schema
    if schema is valid and move = true, move the file
    if schema is invalid remove the file and return an array of invalid file
    if schema is valid and move = false, copy the file
  if schema is not provided
    move = true, move the file 
    move = false, copy the file
    
  
